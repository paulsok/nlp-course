{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npIrOYmGWU7b"
      },
      "source": [
        "# Model Quantization\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2023/week10_efficiency/practice.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "In this session, you're going to implement post-training quantization approaches for _Large Language Models_, ranging from naive ones to State of The Art techniques. The main goal is to implement [GPTQ](https://arxiv.org/abs/2210.17323), with some of it's newer extension left as bonus exercises.\n",
        "\n",
        "<font color='red'>Important note:</font>\n",
        "This homework is designed to run on colab with T4 gpu. It requires at least 15Gb of *VRAM*, 12Gb of *RAM*. If your machine meets those criteria, you should be good to go too.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld5GgOmT2WE1"
      },
      "source": [
        "# Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v6kSVkV2cSS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install transformers==4.35.0\n",
        "%pip install sentencepiece==0.1.99\n",
        "%pip install datasets==2.14.6\n",
        "%pip install accelerate==0.24.1\n",
        "%pip install Ninja==1.11.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtpmhiksi62J"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBpuR3vpi62J"
      },
      "outputs": [],
      "source": [
        "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "%env CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from typing import Mapping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import transformers\n",
        "from transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM\n",
        "from transformers.models.llama.configuration_llama import LlamaConfig\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQkjY8Jp1OpH"
      },
      "source": [
        "# Quantizing Matrices Row-Wise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzEoWTNMi62K"
      },
      "source": [
        "### Basic Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFG1FDsVi62L"
      },
      "source": [
        "**Mapping the values to the allowed range**\n",
        "\n",
        "Quantization is the process of mapping input values from a large set to output values in a smaller set. For instance, if we consider 4-bit\n",
        "quantization, our values are represented by $4$ bits, meaning we can represent values between 0 and $2^4-1=15$.\n",
        "\n",
        " * To produce the quantized representation, we need to be able to map the matrix values to and from this range.\n",
        " * For reasons that become important later, we will perform this mapping independently for each matrix row.\n",
        " * We will parametrize the mapping like this: $out = \\frac{in}{scale} + zero$, where $scale$ and $zero$ are row-wise constants.\n",
        " * For a matrix of size `(m, k)` ($m$ rows, $k$ columns) we will aggregate those parameters into two vectors `scale` and `zero` of size `(m, 1)`.\n",
        "\n",
        "**Task (1pt):** Complete the function below to perform this mapping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1xDv2d12hw9"
      },
      "outputs": [],
      "source": [
        "def get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n",
        "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
        "    min_val = torch.min(x, dim=-1).values\n",
        "    max_val = torch.max(x, dim=-1).values\n",
        "    scale = (max_val - min_val) / max_abs\n",
        "    scale[scale == 0] = 1\n",
        "    zero = - min_val / scale\n",
        "\n",
        "    return scale.unsqueeze(-1), zero.unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ8TZiGyi62M"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "assert scale.shape == (512, 1), \"scale is wrong shape\"\n",
        "assert zero.shape == (512, 1), \"zero is wrong shape\"\n",
        "assert torch.all(scale * 15 <= 1023.1), \"Scale can't be that large. The resulting interval is too wide\"\n",
        "assert torch.all(scale * 15 >= 1022.9), \"Scale shouldn't be that small. The resulting interval is too narrow\"\n",
        "assert torch.all(-0.001 <  x / scale + zero) and torch.all(x / scale + zero < 15 + 0.001)\n",
        "\n",
        "x = torch.zeros(128, 128)\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "assert torch.all(scale == 1) and torch.all(scale * 15 >= 0.99), \"If all the values in a row are identical, let us set scale to 1\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E1DfQaSi62N"
      },
      "source": [
        "**Quantization**\n",
        "\n",
        "Having mapped the values into the allowed range, we can simply round them to obtain the quantized matrix. Complete the functions below to perform row-wise quantization. Note that:\n",
        " * You should `torch.clamp(...)` the quantized values to ensure that they are in the allowed range.\n",
        " * Some functions return the quantized matrix, as well as the quantization constants, because we'll need them to dequantize the matrix. Use `get_scale_and_zero` to obtain the them.\n",
        " * Note that we cast the quantized tensor to `uint8`, but the values themselves must be in the possibly narrower range, as determined by the number of bits. Obviously, we require the latter to be less or equal than 8.\n",
        "\n",
        "**Task (1pt):** Complete the function below to perform quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZR0OZ0ai62N"
      },
      "outputs": [],
      "source": [
        "def quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n",
        "    \"\"\"Quantizes a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "        bits (int): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        Tensor: quantized tensor in uint8\n",
        "    \"\"\"\n",
        "    max_abs = 2 ** bits - 1\n",
        "    quantized_x = torch.round(x / scale + zero)\n",
        "    quantized_x = torch.clamp(quantized_x, 0, max_abs)\n",
        "    return quantized_x.to(torch.uint8)\n",
        "\n",
        "\n",
        "def dequantize(quantized_x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n",
        "    \"\"\"Dequantize a tensor\n",
        "    Args:\n",
        "        quantized_x (Tensor): quantized tensor in uint8\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "\n",
        "    Returns:\n",
        "        Tensor: dequantized tensor\n",
        "    \"\"\"\n",
        "    return quantized_x * scale - zero * scale\n",
        "\n",
        "\n",
        "def measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Determine the values interval mapping parameters and quantize a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        bits (float): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n",
        "    \"\"\"\n",
        "    max_abs = 2 ** bits - 1\n",
        "    scale, zero = get_scale_and_zero(x, max_abs)\n",
        "    x_quantized = quantize(x, scale, zero, bits)\n",
        "    return x_quantized, scale, zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM8eIsR8i62O"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = get_scale_and_zero(x, 15)\n",
        "quantized_x, scale, zero = measure_and_quantize(x, 4)\n",
        "\n",
        "assert quantized_x.shape == x.shape, \"Shape of quantized_x is incorrect\"\n",
        "assert scale.shape == (512, 1), \"Shape of scale is incorrect\"\n",
        "assert zero.shape == (512, 1), \"Shape of zero is incorrect\"\n",
        "assert torch.all(quantized_x >= 0) and torch.all(quantized_x <= 15) and torch.any(quantized_x == 15), \"wrong quantized_x values range\"\n",
        "assert torch.allclose(x, dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBDvbhJSi62O"
      },
      "source": [
        "**Using the quantized matrix**\n",
        "\n",
        "To actually use the matrix, we'll have to map it's values back into their original form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMzGxzt-i62P"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, scale, zero, bias):\n",
        "        super().__init__()\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.scale = nn.Parameter(scale, requires_grad=False)\n",
        "        self.zero = nn.Parameter(zero, requires_grad=False)\n",
        "        self.bias = nn.Parameter(bias.data.clone().detach()) if bias is not None else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, dequantize(self.quantized_weight, self.scale, self.zero), self.bias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncM_JzOii62P"
      },
      "source": [
        "This class will be used as a replacement for `nn.Linear`. It holds the quantized weight and only dequantizes it during it's forward passes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPaMiUC91VYH"
      },
      "source": [
        "# LLM Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HRqro7X1eoT"
      },
      "source": [
        "### Preparations\n",
        "\n",
        "Run all the cells in this subsection to download and prepare the model and the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipghRjSA1jVN"
      },
      "source": [
        "**Downloading the model**\n",
        "\n",
        "Run the code below to download the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIUbMToO3C9B"
      },
      "outputs": [],
      "source": [
        "!mkdir model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9SnAHdc19ap"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "LLAMA_REPO = \"Enoch/llama-7b-hf\"\n",
        "snapshot_download(repo_id=LLAMA_REPO, local_dir=\"./model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKQSag7f1nJC"
      },
      "source": [
        "**Dispatching the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrNGVHZhi62T"
      },
      "source": [
        "To properly quantize the model we'll need two functions.\n",
        " 1. `initialize_layerless_llama` creates a llama model without any layers, but correct weights otherwise\n",
        " 2. `load_and_dispatch_a_layer` loads a layer inserts it into the model after the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls5meN1c4No-"
      },
      "outputs": [],
      "source": [
        "# Disable fancy model initialization since we'll override those values anyway\n",
        "def skip(*args, **kwargs):\n",
        "    pass\n",
        "torch.nn.init.kaiming_uniform_ = skip\n",
        "torch.nn.init.uniform_ = skip\n",
        "torch.nn.init.normal_ = skip\n",
        "\n",
        "\n",
        "def initialize_layerless_llama(checkpoint_path):\n",
        "    config = LlamaConfig.from_pretrained(LLAMA_REPO)\n",
        "    config.num_hidden_layers=0\n",
        "\n",
        "    model = LlamaForCausalLM(config)\n",
        "    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"pytorch_model-00033-of-00033.bin\")))\n",
        "    model.seqlen = 2048\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    return model.to(torch.float16)\n",
        "\n",
        "\n",
        "def load_and_dispatch_a_layer(layer_idx, checkpoint_path, model: LlamaForCausalLM):\n",
        "    if checkpoint_path == \"TEST\":\n",
        "        linear = nn.Linear(16, 16)\n",
        "        linear.weight.data = torch.arange(16 * 16).reshape(16, 16).float()\n",
        "        model.model.layers.append(nn.ModuleDict({\"submodule\": linear}))\n",
        "        return\n",
        "\n",
        "    config = transformers.AutoConfig.from_pretrained(LLAMA_REPO)\n",
        "\n",
        "    layer = LlamaDecoderLayer(config)\n",
        "    layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"pytorch_model-{layer_idx + 1:05}-of-00033.bin\"))\n",
        "    layer_state_dict = {name[len(f\"model.layers.{layer_idx}.\"):]: tensor for name, tensor in layer_state_dict.items()}\n",
        "    layer.load_state_dict(layer_state_dict, strict=False)\n",
        "    del layer_state_dict\n",
        "\n",
        "    model.model.layers.append(layer.to(torch.float16))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsOwpAvbi62U"
      },
      "source": [
        "Calling `initialize_layerless_llama` and then calling `load_and_dispatch_a_layer` for each layer in order would fully load the model, but we'll also quantize the layes as we go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agCFfP7x2Au6"
      },
      "source": [
        "### RTN Quantization for LLaMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwS6bTIwi62V"
      },
      "source": [
        "**Auxiliary functions:**\n",
        " * `find_layers` takes a module and returns a dictionary containing all of it's *Linear* submodules with their path-names as the keys.\n",
        " * `replace_submodule` takes a module, a path-name and a submodule and replaces the module's submodule at path-name with the new submodule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vo0UYKvi62V"
      },
      "outputs": [],
      "source": [
        "def find_layers(module: nn.Module, name='') -> dict[str, nn.Module]:\n",
        "    if type(module) == nn.Linear:\n",
        "        return {name: module}\n",
        "    res = {}\n",
        "    for name1, child in module.named_children():\n",
        "        res.update(find_layers(\n",
        "            child, name=name + '.' + name1 if name != '' else name1\n",
        "        ))\n",
        "    return res\n",
        "\n",
        "\n",
        "def replace_submodule(module, submodule_path, new_submodule):\n",
        "    submodule_names = submodule_path.split(\".\")\n",
        "    for submodule in submodule_names[:-1]:\n",
        "        module = getattr(module, submodule)\n",
        "    setattr(module, submodule_names[-1], new_submodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJALPHCSi62W"
      },
      "source": [
        "**Load-Quantize cycle**\n",
        "\n",
        "First, take a look at the function below. It uses the functions above to load the layers one by one and iterate over their `Linear` submodules replacing them with `QuantizedLinear`. A few things to keep in mind:\n",
        " * Note that the quantization happens on `.cuda()`, because we'll load *LLaMA* in `float16` which is not supported on `cpu`.\n",
        " * We call `torch.cuda.empty_cache()` after processing each layer because we'll have just enough *VRAM* for this to work.\n",
        " * The loaded model is placed in RAM.\n",
        "\n",
        "**Task (1pt):** implement RTN quantization for *LLaMA*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TRwggs3Lb2F"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def llama_rtn(checkpoint_path: os.PathLike, model: LlamaForCausalLM, bits: int):\n",
        "    \"\"\"Loads LLaMA layers one by one and quantizes them with RTN\n",
        "    Args:\n",
        "        checkpoint_path (os.PathLike): folder containing LLaMA weights\n",
        "        model (LlamaForCausalLM): model to dispatch layers into\n",
        "        bits (int): number of bits to quantize to\n",
        "    \"\"\"\n",
        "    # Load and quantize all the layers\n",
        "    layers = model.model.layers\n",
        "    assert len(layers) == 0\n",
        "    for i in trange(32):\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].cuda()\n",
        "\n",
        "        linear_submodules = find_layers(layer)\n",
        "        # Quantize the linear layers and replace the original ones with them\n",
        "        for name, linear in linear_submodules.items():\n",
        "            quantized_weight, scale, zero = measure_and_quantize(linear.weight.data, bits)\n",
        "            quantized_linear = QuantizedLinear(quantized_weight, scale, zero, linear.bias)\n",
        "            replace_submodule(layer, name, quantized_linear)\n",
        "        layers[i] = layer.cpu()\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8FBs1ji62W"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "model = nn.ModuleDict({\"model\": nn.ModuleDict({\"layers\": nn.ModuleList([])})})\n",
        "llama_rtn(\"TEST\", model, 4)\n",
        "\n",
        "assert len(model.model.layers) == 32, \"You didn't load all the layers\"\n",
        "assert all(isinstance(layer.submodule, QuantizedLinear) for layer in model.model.layers), \"Some Linears weren't properly replaced\"\n",
        "assert torch.all(model.model.layers[0].submodule.quantized_weight == torch.arange(16).unsqueeze(0).repeat(16, 1)), \"Quantized weights are weird\"\n",
        "assert torch.all(model.model.layers[0].submodule.scale == 1), \"Quantized scales are weird\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4OlcGF2aEts"
      },
      "source": [
        "### Testing the Quantized Model\n",
        "\n",
        "Now we have everything we need to quantize the _LLaMA-7B_ model to 4 bits. Let us do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luUgnOhxaFcw"
      },
      "outputs": [],
      "source": [
        "MODEL = \"./model/\"\n",
        "SEED = 0\n",
        "BITS = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUD4z4t7a7z8"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n",
        "\n",
        "model = initialize_layerless_llama(MODEL)\n",
        "llama_rtn(MODEL, model, BITS)\n",
        "model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EN9tVawbUfG"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you explain the Pythagorean theorem?\",\n",
        "    \"What is photosynthesis?\",\n",
        "    \"Give me a summary of 'Romeo and Juliet'\",\n",
        "    \"How far is the moon from the Earth?\",\n",
        "    \"What is a haiku?\",\n",
        "]\n",
        "answers = []\n",
        "\n",
        "for question in tqdm(questions):\n",
        "    tokenized_input = tokenizer(\n",
        "        f\"QUESTION: {question}\\n ANSWER:\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            tokenized_input[\"input_ids\"].cuda(),\n",
        "            max_length=50, num_beams=3, early_stopping=True,\n",
        "        )[0]\n",
        "    answer = tokenizer.decode(output, skip_special_tokens=True)\n",
        "    answers.append(answer[:answer.find(\".\")] + \".\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-QFd87GjNvd"
      },
      "outputs": [],
      "source": [
        "print(*answers, sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn-Vko3urrM0"
      },
      "outputs": [],
      "source": [
        "model = model.cpu()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOFpjGxX2H-J"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "Before we start quantizing the model itself, let us create a way to evaluate it's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6Muc5ZyupZ"
      },
      "source": [
        "**Downloading the data**\n",
        "\n",
        "As a metric of the models' performance, we'll use it's PPL on the [wikitext2](https://paperswithcode.com/dataset/wikitext-2) dataset. Let us download and tokenize it. We'll need two subsets of it:\n",
        " * Test set of size ... to evaluate the models.\n",
        " * A train subset of size ... that we'll use later for GPTQ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUSNnguPysaD"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "\n",
        "def get_wikitext2(seed, seqlen, nsamples=128):\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n",
        "\n",
        "    train_input_ids = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt').input_ids\n",
        "    random.seed(seed)\n",
        "    train_batch = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, train_input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = train_input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        train_batch.append(inp[0])\n",
        "\n",
        "    test_input_ids = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt').input_ids\n",
        "    test_input_ids = test_input_ids[:, :(test_input_ids.shape[1] // seqlen) *  seqlen]\n",
        "    test_input_ids = test_input_ids.reshape(test_input_ids.shape[1] // seqlen, seqlen)\n",
        "\n",
        "    return torch.stack(train_batch), test_input_ids\n",
        "\n",
        "train_batch, test_input_ids = get_wikitext2(SEED, 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77M_WBTuOLh"
      },
      "source": [
        "**Model offloading**\n",
        "\n",
        "We want to evaluate the model's performance on a large dataset. The model barely fits on the *GPU*, and we'll have to infer in on long text sequences. We don't have enought *VRAM* to do that.\n",
        "\n",
        "Instead, we'll keep most of the model in *RAM*, only transferring the layers to *GPU* as we go through them one by one, and putting them back when we're done.\n",
        "\n",
        "**Obtaining the first layer inputs**\n",
        "\n",
        "To start iterating over the layers, we'll first have to obtain the fist layer inputs. We use the function below to do it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbxZfwj8ipPF"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/utils.py --no-check-certificate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W50rdH0i62Y"
      },
      "outputs": [],
      "source": [
        "from utils import get_first_layer_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN2TrGEBi62Y"
      },
      "outputs": [],
      "source": [
        "class TestModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleDict({\"layers\": nn.ModuleList([])})\n",
        "\n",
        "    def forward(self, inp):\n",
        "        self.model.layers[0](2 * inp, attention_mask=\"Some Mask\", position_ids=\"Some Ids\")\n",
        "\n",
        "\n",
        "test_model = TestModule()\n",
        "llama_rtn(\"TEST\", test_model, 4)\n",
        "test_inputs = torch.arange(16 * 32).reshape(16, 32).float()\n",
        "hidden_states, attention_mask, position_ids = get_first_layer_inputs(test_model, test_inputs)\n",
        "assert torch.all(2 * test_inputs == hidden_states)\n",
        "assert attention_mask == \"Some Mask\"\n",
        "assert position_ids == \"Some Ids\"\n",
        "assert len(test_model.model.layers) == 32, \"The function doesn't put back the original layers\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFI_nTCXi62Y"
      },
      "source": [
        "**Forward passing layer-by-layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzEPMSsEi62Y"
      },
      "outputs": [],
      "source": [
        "def forward_pass_layer(layer: nn.Module, inps: torch.Tensor, outs: torch.Tensor, attention_mask: Tensor, position_ids: Tensor):\n",
        "    \"\"\"Forward pass inps through the layer ONE INP AT A TIME saving the outputs into the corresponding elements of outs\"\"\"\n",
        "    for j in range(inps.shape[0]):\n",
        "        outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
        "\n",
        "\n",
        "def get_batch_nll(model: nn.Module, batch: Tensor):\n",
        "    # Collect the first layer inputs, put them on .cuda()\n",
        "    inps, attention_mask, position_ids = get_first_layer_inputs(model, batch)\n",
        "    inps = inps.cuda()\n",
        "    attention_mask = attention_mask.cuda()\n",
        "    position_ids = position_ids.cuda()\n",
        "\n",
        "    # Create a buffer for layer outputs\n",
        "    outs = torch.zeros_like(inps)\n",
        "\n",
        "    # Forward pass through the layers\n",
        "    layers = model.model.layers\n",
        "    assert len(layers) == 32\n",
        "    for i in trange(32, leave=False):\n",
        "        layer = layers[i].cuda() # Take a layer and put in on .cuda()\n",
        "\n",
        "        forward_pass_layer(layer, inps, outs, attention_mask, position_ids) # Forward pass a layer\n",
        "        inps, outs = outs, inps # Prepare the inputs and the output buffer for the next layer. Reuse the existing buffers\n",
        "\n",
        "        layers[i] = layer.cpu() # Put layer back on .cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    # Calculate NLL\n",
        "    nll = 0\n",
        "    model.model.norm = model.model.norm.cuda()\n",
        "    model.lm_head = model.lm_head.cuda()\n",
        "    for i in range(inps.shape[0]):\n",
        "        lm_logits = model.lm_head(model.model.norm(inps[i].unsqueeze(0)))\n",
        "        labels = batch[i]\n",
        "        # Calculate the language modeling Negative Log Likelyhood\n",
        "        shift_logits = lm_logits[:, :-1, :]\n",
        "        shift_labels = labels[1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).cuda())\n",
        "        nll += float(loss) * model.seqlen\n",
        "    model.model.norm = model.model.norm.cpu()\n",
        "    model.lm_head = model.lm_head.cpu()\n",
        "    return nll\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, test_input_ids):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    total_nll = 0\n",
        "    for batch in tqdm(torch.tensor_split(test_input_ids, 4)):\n",
        "        total_nll += get_batch_nll(model, batch)\n",
        "\n",
        "    # Calculate PPL\n",
        "    ppl = math.exp(total_nll / test_input_ids.numel())\n",
        "    print(f\"PPL: {ppl}\")\n",
        "    return ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHq3Z8RlLpuc"
      },
      "source": [
        "**Calculating PPL**\n",
        "\n",
        "We've already loaded and quantized the model. All that's left is to evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybS4iAjsq67Z"
      },
      "outputs": [],
      "source": [
        "rtn_ppl = llama_eval(model, test_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "722oBDnwq_1x"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "assert rtn_ppl > 6.3 and rtn_ppl < 6.6\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urb_7OhSq_zA"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CQIk33arN6A"
      },
      "source": [
        "### GPTQ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t16SaWf3rQt7"
      },
      "source": [
        "GPTQ is the State Of The Art quantization algorithm for post-training DL model quantization. It works by sequentially quantizing the model's linear layer weights.\n",
        "\n",
        "Although in outputs results similar to what one would get with Round To Nearest quantization, it makes a key observations to boost it's end performance:\n",
        " * It is layer input aware (also referred to as \"1-Shot\" method), meaning int optimizes the quantized matrix to show best performance on inputs typically encountered in that layer.\n",
        "More formally, the problem can be formulated as:\n",
        "$$\n",
        "W_q = argmin_{\\widehat{W}}\\|XW^T - X\\widehat{W}^T\\|_2^2\n",
        "$$\n",
        ", where\n",
        " * $X$ is the input matrix of shape `(..., IN)`.\n",
        " * $XW^T$ is the unquantized output of shape `(..., OUT)`. We think of the norm above as taking a sum over those (...) dimensions.\n",
        " * $W$ is the unquantized weight of shape `(OUT, IN)`.\n",
        " * $\\widehat{W}$ is the quantized weight taken from some quantization grid.\n",
        "\n",
        "One can notice that the expression above is independent with regard to the rows of $W$ and $\\widehat{W}$, meaning we can solve it for each row in parallel. This is the reason why we're working with row-wise quantization in the first place. Notice that the quantization grid only depends on min/max values withing the row and not the quantization process, so we can think of it as fixed.\n",
        "\n",
        "and the dimension of the optimization problem is `IN`, which is too much to solve exactly. The algorithm proposes to solve it iteratively.\n",
        "\n",
        "Less us consider a vector of full precision weights $F$ and corresponding sent of inputs $X_F$. The corresponding objective is quadratic with Hessian\n",
        "$$\n",
        "H_F = 2X_F^TX_F^.\n",
        "$$\n",
        "The algorithm can be described like this:\n",
        " * Do the following steps until $F$ is fully quantized:\n",
        "    1. Given the next index to quantize $i$, and corresponding unquantized element $F_i$.\n",
        "    2. Quantize the coordinate by prjecting in onto the quantization grid $Q_i = quant(F_i)$.\n",
        "    3. Update all of the remaining weights $F_: = F_: - \\frac{F_i - quant(F_i)}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,:}$.\n",
        "    4. Exclude $i$ from $F$.\n",
        "\n",
        "It uses the inverse Hessian to slightly tune the remaining unquantized weights to mitigate the quantization error.\n",
        "\n",
        "As for how $i$ is chosen, an observation was made that iterating over indices in order of **decreasing diagonal Hessian elements** provides the best performance.\n",
        "\n",
        "There are a few more ideas that make this algorithm much faster:\n",
        " 1. We can represent the order of quantization (selection of $i$) by permuting the row in advance, and then iterating over the row element in order.\n",
        " $$\n",
        "   F_{i:} = F_{i:} - \\frac{F_{i} - quant(F_{i})}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,i:}\n",
        " $$\n",
        " 2. The problem is row-wise independent, meaning that we use the same permutation each row and perform those operations in a vector fashion for all the rows at the same time.\n",
        " $$\n",
        "   F_{:,i:} = F_{:,i:} - \\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\odot\\left[H_F^{-1}\\right]_{i,i:}\\leftarrow\\text{\\textbf{ you'll have to code this}}\n",
        " $$\n",
        " 3. We don't actually need to recompute the inverse Hessian. At $i$-th step we only need its $t$-th row, and we can use fancy math to precompute the matrix containing all of those rows in advance.\n",
        " $$\n",
        "  H^{-1} = Cholesky(H^{-1})^T    \n",
        " $$\n",
        "\n",
        " 4. We don't need to tune all the remaining unquantized values right away. We can only apply the updates for the closest elements right away and accumulate all the other updates to apply them only once in a while.\n",
        "\n",
        "    We'll do this in block of fixed size, applying the updates inside of those blocks and updating the weights outside only when we're done with the block. To accumulate those updates, we'll collect the scaled quantization error\n",
        "    $$\n",
        "      Err_{:,i} =\\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\text{ for all }i\\text{ in block}.\n",
        "    $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogTj9Op5raaD"
      },
      "source": [
        "**GPTQ within blocks**\n",
        "\n",
        "Implement GPTQ within the block. Iterate over the columns in ordered vector fashion, quantizing them one by one and updating all the remaining colums within the block.\n",
        "\n",
        "Return the quantized weight as well as the matrix of quantization errors that we'll need to tune the unquantized weights outside of the block.\n",
        "\n",
        "**Task (2pt):** Implement GPTQ block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8GwQU5CrNv_"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def gptq_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"Perform GPTQ within block\n",
        "    Args:\n",
        "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
        "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
        "        scale (Tensor): row-wise quantization constats of shape (OUT, 1)\n",
        "        zero (Tensor): row-wise quantization constats of shape (OUT, 1)\n",
        "        bits (int): number of bits to quantize() to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
        "    \"\"\"\n",
        "    block_weight = block_weight.clone()\n",
        "    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.uint8, device=block_weight.device)\n",
        "    scaled_block_error = torch.zeros_like(block_weight)\n",
        "\n",
        "    # Interate over the block's columns\n",
        "    for i in range(block_weight.shape[1]):\n",
        "        # Get the column and the corresponding inverse Hessian\n",
        "        column_weight = block_weight[:, [i]]\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_block_weight, scaled_block_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgRy6B_didyY"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "!wget -O gptq_block_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_weight_reference.pt --no-check-certificate\n",
        "!wget -O gptq_block_error_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_error_reference.pt --no-check-certificate\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(0)\n",
        "\n",
        "weight = torch.rand((128, 128), generator=generator).cuda()\n",
        "scale, zero = get_scale_and_zero(weight, 15)\n",
        "\n",
        "block_weight = weight[:,:16]\n",
        "\n",
        "block_hessian_inverse = (torch.triu(torch.rand((16, 16), generator=generator), diagonal=1) + torch.diag(torch.rand(16, generator=generator) + 1)).cuda()\n",
        "quantized_block_weight, scaled_block_error = gptq_block(block_weight, block_hessian_inverse, scale, zero, 4)\n",
        "\n",
        "assert torch.all(quantized_block_weight == torch.load(\"gptq_block_weight_reference.pt\"))\n",
        "assert torch.allclose(scaled_block_error, torch.load(\"gptq_block_error_reference.pt\"), rtol=1e-5, atol=1e-06)\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXmzbg8y18R"
      },
      "source": [
        "**Now we can implement the full algorithm:**\n",
        " * Get row-wise quantization constants.\n",
        " * Sort the columns by decreasing Hessian diagonal values. Think about how you'd have to permute the Hessian as well.\n",
        " * Process the Hessian to obtain the precomputed inverse Hessian.\n",
        " * Iterate over the columns in blocks:\n",
        "    * Get the next block and quantize it.\n",
        "    * Tune all the following blocks to mitigate the quantization error.\n",
        "      $$\n",
        "         F_{:,block\\_end:} = F_{:,block\\_end:} - Err_{:,block\\_start:block\\_end}\\odot\\left[H_F^{-1}\\right]_{block\\_start:block\\_end,block\\_end:}\n",
        "      $$\n",
        " * Restore the original order for quantized columns.\n",
        "\n",
        "**Task (2pt):** implement the full algorithms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqI4FamprFPE"
      },
      "outputs": [],
      "source": [
        "def prepare_inverse_hessian(hessian: Tensor, percdamp: float) -> Tensor:\n",
        "    \"\"\"Precomputes inverse Hessian\n",
        "    Args:\n",
        "        hessian (Tensor): problem hessian\n",
        "        percdamp (float): diagonal damping constant for numerical stability\n",
        "    Returns:\n",
        "        Tensor: precomputed inverse Hessian\n",
        "    \"\"\"\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(hessian.shape[0], device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    return hessian\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def gptq(weight: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Quantizes weight with GPTQ\n",
        "    Args:\n",
        "        weight (torch.Tensor): weight to quantize\n",
        "        bits (int): number of bits to quantize to\n",
        "        hessian (torch.Tensor): problem Hessian\n",
        "        blocksize (int, optional): Defaults to 128.\n",
        "        percdamp (float, optional): Hessian damping constant for numerical stability. Defaults to .01.\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized_weight, row-wise quantization scales, row-wise quantization zeroes\n",
        "    \"\"\"\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "    num_columns = weight.shape[1]\n",
        "    hessian = hessian.float()\n",
        "\n",
        "    # Identify and patch always-zero input coordinates\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Get row-wise quantization constants\n",
        "    scale, zero = get_scale_and_zero(weight, 2 ** bits - 1)\n",
        "\n",
        "    # Sort the columns by decreasing Hessian diagonal values.\n",
        "    # Transform the hessian accordingly.\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    perm =\n",
        "    invperm =\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
        "    hessian_inverse = prepare_inverse_hessian(hessian, percdamp)\n",
        "\n",
        "    # Iterate over the columns in blocks\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.uint8, device=weight.device)\n",
        "    for block_start in range(0, num_columns, blocksize):\n",
        "\n",
        "        block_end = min(block_start + blocksize, num_columns)\n",
        "\n",
        "        # Get the next block and quantize it\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "\n",
        "        # Tune all the following blocks to mitigate the quantization error\n",
        "\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    # Reverse the permutation of the quantized weight\n",
        "    quantized_weight = quantized_weight[:, invperm]\n",
        "\n",
        "    return quantized_weight, scale.to(dtype), zero.to(dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI1R_stdrFMk"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "!wget -O gptq_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_weight_reference.pt --no-check-certificate\n",
        "\n",
        "generator = torch.Generator()\n",
        "generator.manual_seed(0)\n",
        "\n",
        "hessian = torch.triu(torch.rand((128, 128), generator=generator) + 4 * torch.eye(128)).cuda().half()\n",
        "hessian += hessian.clone().T\n",
        "quantized_weight, _, _ = gptq(weight, 4, hessian, 32)\n",
        "\n",
        "assert torch.all(quantized_weight == torch.load(\"gptq_weight_reference.pt\"))\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHAnRizBy8HC"
      },
      "source": [
        "**Sequential Model Quantization**\n",
        "\n",
        "The GPT quantization approach implemented here requires an ordered approach due to its input-dependent nature. For each `Linear` submodule within the GPT model, we need to ensure that the input data is representative of the actual operating conditions post-quantization. This involves propagating a batch of input samples through the model sequentially, with each layer's input being the output of the preceding **quantized** layers.\n",
        "\n",
        "The quantization process must follow a strict sequence both across and within layers. Within each layer, there is a predetermined order in which the submodules must be quantized, which is dictated by the dependencies between them. This order is defined by the \"sequential groups\".\n",
        "\n",
        "The steps of the algorithm are as follows:\n",
        "- Retrieve and prepare inputs for the first layer.\n",
        "- Iterate through each layer in the model:\n",
        "  - Load the current layer for processing.\n",
        "  - Within each layer, process the sequential groups of submodules:\n",
        "    - Attach forward hooks to collect input data to each submodule.\n",
        "    - Execute a forward pass through the layer to accumulate the necessary input data for quantization.\n",
        "    - Remove the hooks after data collection.\n",
        "    - Apply GPTQ to quantize the submodule weights using the accumulated input data.\n",
        "  - Perform another forward pass through the quantized layer to generate the inputs for the next layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMyrGkHqy-xg"
      },
      "outputs": [],
      "source": [
        "def get_accumulate_input_fn(name: str, hessians: Mapping[str, Tensor], num_samples: Mapping[str, int]):\n",
        "    \"\"\"Generate a callback that updates the corresponding hessians and counts when given input\n",
        "    Args:\n",
        "        name (str): module name\n",
        "        hessians (Mapping[str, Tensor]): a dict of modules' hessians, accessible by module name\n",
        "        num_samples (Mapping[str, int]): a dict of callback call counters\n",
        "    \"\"\"\n",
        "    def tmp(_, inp, out):\n",
        "        inp = inp[0].data # ... x hidden_size\n",
        "        inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "        inp = inp.t().float() # hidden_size x inputs\n",
        "        num_samples[name] += 1\n",
        "        if hessians[name] is None:\n",
        "            hessians[name] = inp.matmul(inp.t())\n",
        "        else:\n",
        "            hessians[name] += inp.matmul(inp.t())\n",
        "    return tmp\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_gptq(checkpoint_path: os.PathLike, model: LlamaForCausalLM, batch: Tensor, bits: int):\n",
        "    \"\"\"Loads LLaMA layers one by one and quantizes them with GPTQ\n",
        "    Args:\n",
        "        checkpoint_path (os.PathLike): folder containing LLaMA weights\n",
        "        model (LlamaForCausalLM): model to dispatch layers into\n",
        "        batch (Tensor): sample model inputs\n",
        "        bits (int): number of bits to quantize to\n",
        "    \"\"\"\n",
        "    # Collect the first layer inputs, put them on .cuda() (the same as in get_batch_nll)\n",
        "    inps, attention_mask, position_ids = get_first_layer_inputs(model, batch)\n",
        "    inps = inps.cuda()\n",
        "    attention_mask = attention_mask.cuda()\n",
        "    position_ids = position_ids.cuda()\n",
        "\n",
        "    # Create a buffer for layer outputs\n",
        "    outs = torch.zeros_like(inps)\n",
        "\n",
        "    # Forward pass through the layers\n",
        "    layers = model.model.layers\n",
        "    assert len(layers) == 0\n",
        "    for i in trange(32):\n",
        "        # Load and dispatch the next layer\n",
        "        load_and_dispatch_a_layer(i, checkpoint_path, model)\n",
        "        layer = layers[i].cuda()\n",
        "        linear_layers = find_layers(layer)\n",
        "\n",
        "        hessians = {name: None for name in linear_layers}\n",
        "        num_samples = {name: 0 for name in linear_layers}\n",
        "        handles = [\n",
        "            linear_layers[name].register_forward_hook(\n",
        "                get_accumulate_input_fn(name, hessians, num_samples)\n",
        "            ) for name in linear_layers\n",
        "        ]\n",
        "        forward_pass_layer(layer, inps, outs, attention_mask, position_ids)\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        for name, linear in linear_layers.items():\n",
        "            q, scale, zero = gptq(linear.weight.data, bits, 2 * hessians[name] / num_samples[name])\n",
        "            quantized_linear = QuantizedLinear(q, scale, zero, linear.bias)\n",
        "            replace_submodule(layer, name, quantized_linear)\n",
        "\n",
        "        forward_pass_layer(layer, inps, outs, attention_mask, position_ids)\n",
        "        inps, outs = outs, inps\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6lN_q-0zFej"
      },
      "source": [
        "**Evaluating the model with GPTQ**\n",
        "\n",
        "**Task (2pt):** achieve the required PPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKRuiX7Vza96"
      },
      "outputs": [],
      "source": [
        "model = initialize_layerless_llama(MODEL)\n",
        "llama_gptq(MODEL, model, train_batch, BITS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ARPLErza7N"
      },
      "outputs": [],
      "source": [
        "gptq_ppl = llama_eval(model, test_input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QwMLEqUza43"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "assert gptq_ppl < 6.2\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18BHd4Nnza2T"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KggEz3g37B1g"
      },
      "source": [
        "# Dense Integer Packing\n",
        "\n",
        "For the most part, we used `torch.uint8` to represent tensors quantized to *4 bits*. We encoded each *4-bit* value with an *8-bit* value, using twice as much memory as we really needed. We did this because `torch` lacks support for 4-bit tensors. However, we never performed any native operations in *8 bits*; we always converted it to `torch.float16` first. That means we can design a more efficient way to encode *4-bit* tensors and convert to and from it.\n",
        "\n",
        "Your task is to implement these conversions, converting *8-bit* tensors containing *4-bit* values (the way we used to store quantized weights) to and from smaller *8-bit* tensor, utilizing the whole range of values. Moreover, you'll have to do it in a way such that memory representation of contiguous arrays wouldn't change. That is, adjacent columns get squashed together.\n",
        "\n",
        "**Task (1pt):** Implement dense *int4*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-jaEhv57MC9"
      },
      "outputs": [],
      "source": [
        "def dense_pack_4_8(x: Tensor) -> Tensor:\n",
        "    \"\"\"Constructs a densely packed int tensor\n",
        "    Args:\n",
        "        x (Tensor): uint8 with uint4 values range\n",
        "        real_bits (int): real values range of the tensor\n",
        "\n",
        "    Returns:\n",
        "        Tensor: twice as small uint8 tensor with uint8 values range\n",
        "    \"\"\"\n",
        "    x = x.clone().detach()\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "def dense_unpack_4_8(x: Tensor) -> Tensor:\n",
        "    \"\"\"Deconstructs a densely packed int tensor\n",
        "    Args:\n",
        "        x (Tensor): uint8 tensor with uint8 values range\n",
        "\n",
        "    Returns:\n",
        "        Tensor: twice as large int8 tensor with uint4 values range\n",
        "    \"\"\"\n",
        "    x = x.clone().detach()\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sM4tAOU7Oez"
      },
      "outputs": [],
      "source": [
        "# Testing your code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "quantized_x, scale, zero = measure_and_quantize(x, 4)\n",
        "dense_quantized_x = dense_pack_4_8(quantized_x)\n",
        "undense_quantized_x = dense_unpack_4_8(dense_quantized_x)\n",
        "\n",
        "assert dense_quantized_x.shape == (512, 512)\n",
        "assert torch.all(undense_quantized_x == quantized_x)\n",
        "assert (dense_quantized_x // 16 == dense_quantized_x % 16).float().mean() > 0.95, \"close values are supposed to be packed together\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E14awThL68JM"
      },
      "source": [
        "# Bonus: QUIK\n",
        "\n",
        "[QUIK](https://arxiv.org/abs/2310.09259) is an extension of GPTQ. It's main feature is that, in addition to model quantization, it also quantizes activations. That way, multiplication can be performed in `int`, leading to 2x-3x improvement in inference speed over GPTQ, which dequantizes the weights and performs multiplication in `float`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JipC2pi-idyg"
      },
      "source": [
        "### Different Range, Different Scales and Zeros\n",
        "\n",
        "At basic quantization level, QUIK already has a number of differences compared to GPTQ:\n",
        " 1. QUIK quantizes the weights to **signed** integer values for better stability of matrix multiplication.\n",
        " 2. QUIK enforces `zero` to be integer, to be able to perform `int` multiplication with it as well.\n",
        " 3. QUIK changes the sign of `zero` (compare `quik_dequantize` with `dequantize`) for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEtnoIS5R8Z8"
      },
      "outputs": [],
      "source": [
        "def quik_get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n",
        "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
        "    if x.shape[-1] == 0:\n",
        "        return torch.ones(x.shape[:-1], dtype=x.dtype, device=x.device), torch.zeros(x.shape[:-1], dtype=torch.int, device=x.device)\n",
        "    xmin = x.min(-1)[0]\n",
        "    xmax = x.max(-1)[0]\n",
        "\n",
        "    scale = (xmax - xmin) / max_abs / 2\n",
        "    scale[scale == 0] = 1\n",
        "    zero = torch.round((xmax + xmin) / 2 / scale) # zero is int, since we want to use it in int operations\n",
        "\n",
        "    return scale.unsqueeze(-1), zero.unsqueeze(-1).to(torch.int)\n",
        "\n",
        "\n",
        "def quik_quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n",
        "    \"\"\"Given a tensor x quantize it, producing tensors quantized_x of torch.int8 dtype\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "        bits (int): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        Tensor: quantized tensor in int8\n",
        "    \"\"\"\n",
        "    if x.shape[-1] == 0:\n",
        "        return torch.zeros(x.shape, dtype=torch.int8, device=x.device)\n",
        "    max_abs = 2 ** (bits - 1) - 1\n",
        "    quantized_x = torch.round(x / scale) - zero\n",
        "    quantized_x = torch.clamp(quantized_x, -max_abs, max_abs) # what are the allowed values for int8?\n",
        "    return quantized_x.to(torch.int8)\n",
        "\n",
        "\n",
        "def quik_dequantize(x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n",
        "    \"\"\"Dequantize a tensor\n",
        "    Args:\n",
        "        quantized_x (Tensor): quantized tensor in int8\n",
        "        scale (Tensor): values interval mapping scale\n",
        "        zero (Tensor): values interval mapping zero\n",
        "\n",
        "    Returns:\n",
        "        Tensor: dequantized tensor\n",
        "    \"\"\"\n",
        "    return scale * x + scale * zero\n",
        "\n",
        "\n",
        "def quik_measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n",
        "    \"\"\"Determine the values interval mapping parameters and quantize a tensor\n",
        "    Args:\n",
        "        x (Tensor): tensor to quantize\n",
        "        bits (float): number of bits to quantize to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n",
        "    \"\"\"\n",
        "    max_abs = 2 ** (bits - 1) - 1\n",
        "    scale, zero = quik_get_scale_and_zero(x, max_abs)\n",
        "    x_quantized = quik_quantize(x, scale, zero, bits)\n",
        "    return x_quantized, scale, zero\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4zWRo8tidyh"
      },
      "outputs": [],
      "source": [
        "# Testing my code\n",
        "\n",
        "x = torch.arange(512 * 1024).reshape(512, 1024).float()\n",
        "scale, zero = quik_get_scale_and_zero(x, 15)\n",
        "quantized_x, scale, zero = quik_measure_and_quantize(x, 4)\n",
        "\n",
        "assert scale.shape == (512, 1), \"Shape of scale is incorrect\"\n",
        "assert zero.shape == (512, 1), \"Shape of zero is incorrect\"\n",
        "assert torch.allclose(x, quik_dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSqZunEhR-_Y"
      },
      "source": [
        "### Weight Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xa5TnPD1idyi"
      },
      "source": [
        "**Block quantization**\n",
        "\n",
        "On block level, QUIK is identical to GPTQ. Paste your `gptq_block` solution here and simply replace `quantize` with `quik_quantize` and `dequantize` with `quik_dequantize`.\n",
        "\n",
        "**Task (1pt):** Paste GPTQ code into QUIK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaVrG9jDSFZj"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def quik_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n",
        "    \"\"\"NOTE: This function is allowed to alter the block_weight as we won't need those weights anymore\n",
        "\n",
        "    Args:\n",
        "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
        "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
        "        scale (Tensor): row-wise quantization constants of shape (OUT, 1)\n",
        "        zero (Tensor): row-wise quantization constants of shape (OUT, 1)\n",
        "        bits (int): number of bits to quantize() to\n",
        "\n",
        "    Returns:\n",
        "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
        "    \"\"\"\n",
        "    block_weight = block_weight.clone()\n",
        "    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.int8, device=block_weight.device)\n",
        "    scaled_block_error = torch.zeros_like(block_weight)\n",
        "\n",
        "    # Iterate over the block's columns\n",
        "    for i in range(block_weight.shape[1]):\n",
        "        # Get the column and the corresponding inverse Hessian\n",
        "        column_weight = block_weight[:, [i]]\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_block_weight, scaled_block_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Vrjo7qidyj"
      },
      "source": [
        "**Full quantization**\n",
        "\n",
        "To determine the outliers, QUIK needs additional information about layer inputs. Namely, `l_inf_norms` - max module values for each input coordinate in a minibatch. We can see how it's used to extract outliers.\n",
        "\n",
        "The rest of the function, again, is copy-paste from `gptq(...)`. Feel free to reuse your code, but don't forget to replace `gptq_block` with `quik_block`.\n",
        "\n",
        "**Task (1pt):** Paste GPTQ code into QUIK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7TGlfLgidyj"
      },
      "outputs": [],
      "source": [
        "def quik(weight: torch.Tensor, bits: int, hessian: torch.Tensor, l_inf_norms: torch.Tensor, blocksize:int=128, percdamp:float=.01, n_outliers=128):\n",
        "    dtype = weight.dtype\n",
        "    weight = weight.clone().detach()\n",
        "    weight = weight.float()\n",
        "\n",
        "    # Identify and patch always-zero input coordinates\n",
        "    dead = torch.diag(hessian) == 0\n",
        "    hessian[dead, dead] = 1\n",
        "    weight[:, dead] = 0\n",
        "\n",
        "    # Identify outliers by decreasing l_inf_norms. Sort the remained by decrasing hessian values\n",
        "    perm = torch.argsort(l_inf_norms, descending=True)\n",
        "    perm[n_outliers:] = perm[n_outliers:][torch.argsort(torch.diag(hessian)[perm][n_outliers:], descending=True)]\n",
        "    weight = weight[:, perm]\n",
        "    hessian = hessian[perm, :][:, perm]\n",
        "\n",
        "    # Process outliers\n",
        "    outlier_weight = weight[:,:n_outliers]\n",
        "    weight = weight[:,n_outliers:]\n",
        "    num_columns = weight.shape[1]\n",
        "    hessian = hessian[n_outliers:,:][:,n_outliers:]\n",
        "\n",
        "    max_abs = 2 ** (bits - 1) - 1\n",
        "    scale, zero = quik_get_scale_and_zero(weight, max_abs)\n",
        "\n",
        "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
        "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
        "    diag = torch.arange(num_columns, device=weight.device)\n",
        "    hessian[diag, diag] += damp\n",
        "    hessian = torch.linalg.cholesky(hessian)\n",
        "    hessian = torch.cholesky_inverse(hessian)\n",
        "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
        "    hessian_inverse = hessian\n",
        "\n",
        "    # Iterate over the columns in blocks\n",
        "    quantized_weight = torch.zeros(weight.shape, dtype=torch.int8, device=weight.device)\n",
        "    for block_start  in range(0, num_columns, blocksize):\n",
        "        block_end = min(block_start + blocksize, num_columns)\n",
        "\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        # Get the next block and quantize it\n",
        "\n",
        "        # Tune all the following blocks to mitigate the quantization error\n",
        "\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "    return quantized_weight, scale.to(dtype), zero, outlier_weight.to(dtype), perm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GAoLdmOSF6-"
      },
      "source": [
        "### QUIK Linear Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbaTV0rpidyk"
      },
      "source": [
        "**Install CUDA 12.3 and cutlass**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LceebQnjidyk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n",
        "!sudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-toolkit-12-3\n",
        "!pip install --upgrade pip\n",
        "\n",
        "!git clone https://github.com/NVIDIA/cutlass.git\n",
        "!export CUDACXX=/usr/local/cuda/bin/nvcc\n",
        "!cd cutlass && mkdir build && cd build && cmake .. -DCUTLASS_NVCC_ARCHS=75"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3l9KCjuidyl"
      },
      "source": [
        "**Loading custom CUDA kernels**\n",
        "\n",
        "Along with this jupyter notebook, two `CUDA` files are provided, implementing efficient matrix multiplication for `int4` and `int8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF1ZAk6Didyl"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cpp --no-check-certificate\n",
        "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cu --no-check-certificate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXqhFBw1idym"
      },
      "outputs": [],
      "source": [
        "from torch.utils.cpp_extension import load\n",
        "\n",
        "custom_kernel = load(name='custom_kernel', sources=['kernel.cpp', 'kernel.cu'], extra_include_paths=[r\"cutlass/include\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RIy26cHidym"
      },
      "source": [
        "Usage: `output = custom_kernel.int8_matmul(X, Y)` and `output = custom_kernel.int4_matmul(X, Y)` compute $XY^T$ (same as `nn.functional.linear`).\n",
        "\n",
        "Notice that `int8_matmul` takes the normal `torch.int8` tensors, but int4_matmul expects `int4` values densely packed into `torch.uint8` tensors. This is exactly what we wrote **Dense Integer Packing** for. They have a <font color='red'>severe limitation</font>: the dimension of multiplication must be divisible by 16.\n",
        "\n",
        "Now you have to implement the quantized forward pass:\n",
        "\n",
        " $$\n",
        " \\begin{align}\n",
        "    XW^T &= (Q_x \\cdot scale_x + zero_x \\cdot scale_x)(Q_w \\cdot scale_w + zero_w \\cdot scale_w)^T =\\\\\n",
        "    &= (Q_x  + zero_x)(Q_w + zero_w)^T \\cdot (scale_x \\odot scale_w^T) =\\\\\n",
        "    &= (Q_xQ_w^T + Q_x zero_w^T + zero_x Q_w^T + zero_x zero_w^T) \\cdot (scale_x \\odot scale_w^T)\n",
        " \\end{align}\n",
        " $$\n",
        "\n",
        "because of the kernel limitations mentioned above, only the largest integer multiplication ($Q_xQ_w^T$) can be done in `int`. Perform the other ones in `float`.\n",
        "\n",
        "**Task (3pt)**: implement QUIK forward pass and measure it's performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WG8Y2aP4wDMK"
      },
      "outputs": [],
      "source": [
        "class QuikLinear(nn.Module):\n",
        "    def __init__(self, quantized_weight, weight_scale, weight_zero, outlier_weight, bias, bits: int, perm):\n",
        "        super().__init__()\n",
        "        self.bits = bits\n",
        "        self.perm = perm\n",
        "        self.n_outliers = outlier_weight.shape[1]\n",
        "\n",
        "        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
        "        self.weight_scale = nn.Parameter(weight_scale, requires_grad=False)\n",
        "        self.weight_zero = nn.Parameter(weight_zero, requires_grad=False)\n",
        "\n",
        "        self.outlier_weight = nn.Parameter(outlier_weight, requires_grad=False)\n",
        "        self.weights_reduced = self.quantized_weight.to(torch.int32).sum(dim=1).float()\n",
        "\n",
        "        if bias is not None:\n",
        "            self.bias = nn.Parameter(bias.data.clone().detach())\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        out_size, in_size = self.quantized_weight.shape\n",
        "        input = input[...,self.perm]\n",
        "        input_quantized, input_scale, input_zero = quik_measure_and_quantize(input[...,self.n_outliers:], self.bits)\n",
        "\n",
        "        outliers_result = F.linear(input[...,:self.n_outliers], self.outlier_weight, self.bias)\n",
        "        if input_quantized.shape[-1] == 0:\n",
        "            return outliers_result\n",
        "\n",
        "        # Convert necessary components to float\n",
        "        inputs_reduced = input_quantized.to(torch.int32).sum(dim=-1).float()\n",
        "        input_zero = input_zero.float()\n",
        "        weight_zero = self.weight_zero.data.float()\n",
        "\n",
        "        # Fully int operations\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        quantized_result =\n",
        "        quantized_result = quantized_result.float()\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "        # I wish those were int, but float operations\n",
        "        # YOUR CODE HERE>>>>>>>>>\n",
        "        quantized_result +=\n",
        "        quantized_result +=\n",
        "        quantized_result +=\n",
        "        quantized_result =\n",
        "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "        quantized_result = quantized_result.to(torch.float16)\n",
        "        results = quantized_result + outliers_result\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU-aPRtcctTT"
      },
      "source": [
        "### Loading the First LLaMA Layer Attention Q Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CU8tqdd0z0h"
      },
      "outputs": [],
      "source": [
        "model = initialize_layerless_llama(MODEL)\n",
        "inps, _, _ = get_first_layer_inputs(model, train_batch)\n",
        "inps = inps.cuda()\n",
        "model.model.layers = nn.ModuleList()\n",
        "load_and_dispatch_a_layer(0, \"./model\", model)\n",
        "layer = model.model.layers[0].self_attn.q_proj.cuda()\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDouw6RJctTT"
      },
      "source": [
        "### Gathering the Layer Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hsx8WPe1Kek"
      },
      "outputs": [],
      "source": [
        "class AccumulatedInput:\n",
        "    hessians = None\n",
        "    num_samples = 0\n",
        "    actnorms = None\n",
        "\n",
        "@torch.no_grad()\n",
        "def accumulate_layer_input(_, inp, out):\n",
        "    inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n",
        "    inp = inp.t().float() # hidden_size x inputs\n",
        "    AccumulatedInput.num_samples += 1\n",
        "    if AccumulatedInput.hessians is None:\n",
        "        AccumulatedInput.hessians = inp.matmul(inp.t())\n",
        "        AccumulatedInput.actnorms = inp.abs().amax(dim=1)\n",
        "    else:\n",
        "        AccumulatedInput.hessians += inp.matmul(inp.t())\n",
        "        AccumulatedInput.actnorms = torch.maximum(AccumulatedInput.actnorms, inp.abs().amax(dim=1))\n",
        "\n",
        "\n",
        "for inp in inps:\n",
        "    accumulate_layer_input(None, inp, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frvUz5b51Of3"
      },
      "outputs": [],
      "source": [
        "OUT = layer.weight.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWC3HT2D1Pu0"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reference = torch.stack(tuple(layer(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoLpGhzLctTT"
      },
      "source": [
        "### Benchmarking MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI6ShN5gZby9"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero = measure_and_quantize(layer.weight.data, 8)\n",
        "    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    rtn_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", rtn_mse)\n",
        "\n",
        "assert rtn_mse < 8e-5 and rtn_mse > 7e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AifKxdLHZhii"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero = gptq(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples)\n",
        "    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    gptq_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", gptq_mse)\n",
        "\n",
        "assert gptq_mse < 3e-5 and gptq_mse > 1.5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBAhlxPcF0Mv"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples, AccumulatedInput.actnorms, n_outliers=0)\n",
        "    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    quik_0_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", quik_0_mse)\n",
        "\n",
        "assert quik_0_mse < 6e-5 and quik_0_mse > 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euq4H-VwZj9Q"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "with torch.no_grad():\n",
        "    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples, AccumulatedInput.actnorms, n_outliers=256)\n",
        "    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n",
        "\n",
        "    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n",
        "\n",
        "    quik_256_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n",
        "    print(\"MSE:\", quik_256_mse)\n",
        "\n",
        "assert quik_256_mse < 4.25e-5 and quik_256_mse > 3.25e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J0ychH4GDUk"
      },
      "outputs": [],
      "source": [
        "fruits = ['GPTQ', 'QUIK 256 outliers', 'QUIK 0 outliers', 'RNT']\n",
        "counts = [gptq_mse, quik_256_mse, quik_0_mse, rtn_mse]\n",
        "bar_labels = ['green', 'blue', 'blue', 'red']\n",
        "bar_colors = ['green', 'blue', 'blue', 'red']\n",
        "\n",
        "plt.bar(fruits, counts, label=bar_labels, color=bar_colors)\n",
        "plt.ylabel(\"Layer MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF78dQBVIASz"
      },
      "source": [
        "As we can see, QUIK allows for errors smaller than RTN despite utilizing quantized activations, and addition of outliesr further decreases the error, bringing it closer to GPTQ. As we have seen in `benchmark.ipynb`, `int` matmul can lead to 2x-3x speedup over `float16`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFp2mztPGYY4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}